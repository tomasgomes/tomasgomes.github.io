---
layout: post
title: "Topics at the recent KS Single Cell Biology conference (according to Twitter)"
date:   2019-01-26 10:01:00 +0000
categories: rblogging
tags: [single-cell, conference, keystone, twitter]
---


I was really keen on trying to play with some Twitter data. And when the
Keystone Single Cell Biology (\#KSsinglecell) came along and I didn’t
attend, I thought it would be a great way to get a (very crude) summary
of what happened there. And, for future reference,
[this](https://tm4ss.github.io/docs/index.html) looks like a great
resource for text mining using R. I took some concepts from here, but
definitely should have a better look.

We’ll start by loading all required packages

``` r
library(rtweet) # mine Twitter
library(tm) # manipulate text data
library(dplyr) # manipulate data
library(stringr) # string operations
library(markovchain) # self explanatory
library(wordcloud) #self explanatory
library(igraph) # network business
library(ggplot2) # plotting
library(cowplot) # multiplotting

theme_set(theme_minimal()) # pretty plotting
```

To collect the data we first have to setup the Twitter API (see guide
[here](https://rtweet.info/articles/auth.html)). We then extract all
tweets (from the last ~9 days in the free API version) with the
\#KSsinglecell hashtag.

``` r
# AppName
appname = "myapp"

consumer_key ='c_key'
consumer_secret ='c_secret'
access_token ='a_token'
access_secret ='a_secret'

# create token named "twitter_token"
twitter_token = create_token(app = appname,
                             consumer_key = consumer_key, consumer_secret = consumer_secret,
                             access_token = access_token, access_secret = access_secret)

# Retrieve tweets for a particular hashtag
## don't forget the time limit (~7-9 days) for free usage of the twitter API!
## these were collected on 20/01/2019
r_stats <- search_tweets("#KSsinglecell", n = 1000000, include_rts = T, 
                         type = "recent", token = twitter_token)

#saveRDS(r_stats, "KS_tweets.RDS")
```

But they can just be loaded from [here](KStweets_files/figure-markdown_github/KS_tweets.RDS)

``` r
r_stats = readRDS("KS_tweets.RDS")
```

We can see the twitter activity of this hashtag mostly localised during
the conference times.

``` r
ts_df = ts_data(r_stats, "30 mins")
ggplot()+
  geom_line(data = ts_df, mapping = aes(x = time, y = n))+
  geom_vline(xintercept = c(as.numeric(ts_df$time[ts_df$time=="2019-01-13 23:00:00 UTC"]), 
                            as.numeric(ts_df$time[ts_df$time=="2019-01-18 06:00:00 UTC"])), 
             linetype = "dashed", size = 1)+
  labs(x = NULL, y = NULL,
       title = "Frequency of #KSsinglecell tweets in 30 min intervals",
       subtitle = "Dashed lines limit the conference start and end")+
  theme(plot.title = ggplot2::element_text(face = "bold"))
```

<img src="KStweets_files/figure-markdown_github/unnamed-chunk-4-1.png" style="display: block; margin: auto;" />

Lets now check who talked more and who was more talked about. We will be
using the screen names rather than the actual names since these can have
characters that can be hard to handle.

``` r
pop_list = list(
"User Tweeted" = data.frame(table(r_stats$screen_name)),
"User was retweeted" = data.frame(table(r_stats$retweet_screen_name)),
"User was mentioned" = data.frame(table(unlist(r_stats$mentions_screen_name))))

plt_list = list()
for(n in names(pop_list)){
  plot_df = pop_list[[n]]
  plot_df = plot_df[order(plot_df$Freq, decreasing = T),][1:20,]
  plot_df$Var1 = factor(plot_df$Var1, levels = rev(plot_df$Var1))
  plt_list[[n]] = ggplot(plot_df, aes(x = Var1, y = Freq))+
    geom_bar(stat = "identity")+
    coord_flip()+
    labs(x = n, y = "# times")+
    theme(aspect.ratio = 3/1,
          axis.text = element_text(size = 10))
}

plot_grid(plotlist = plt_list, ncol = 3)
```

<img src="KStweets_files/figure-markdown_github/unnamed-chunk-5-1.png" style="display: block; margin: auto;" />

We can clearly identify users that has presented in the conference
amongst the most retweeted and metnioned, as well as the Keystone
Symposium account itself.

For now we have included retweets which help us measure user
“popularity”, but to focus on relevant terms we will be removing these.

``` r
r_stats_unique = r_stats[!r_stats$is_retweet,]
```

There are other hashtags associated with some tweets, and these can tell
us more about what was going on at the conference

``` r
ht_df = data.frame(table(tolower(unlist(r_stats_unique$hashtags))))
ht_df = ht_df[order(ht_df$Freq, decreasing = T),][-1,] # we remove the hashtag we used to make the search
ht_df$Var1 = factor(ht_df$Var1, levels = ht_df$Var1)

ggplot(ht_df, aes(x = Var1, y = Freq))+
    geom_bar(stat = "identity")+
    labs(x = "Hashtag used", y = "# times")+
    theme(aspect.ratio = 1/3,
          axis.text = element_text(size = 10),
          axis.text.x = element_text(angle = 40, hjust = 1, vjust = 1, colour = "black"))
```

<img src="KStweets_files/figure-markdown_github/unnamed-chunk-7-1.png" style="display: block; margin: auto;" />

So, this was a single cell biology conference in Breckenridge, CO, which
is known for the beautiful snowy mountain landscape (and hard to spell).
And it shows on the tweets! Beyond that, we can also see some importance
of epigenetic-related talks, two widely used programming languages in
the field (R and python), and somereferences to imaging and microscopy.
But maybe the actual txt from the tweets will be more informative. We
start by cleaning the text so we can work with the more meaningful
words. For more on this, see [this
website](http://www.rdatamining.com/docs/twitter-analysis-with-r).

``` r
# build a corpus, and specify the source to be character vectors
twCorpus = Corpus(VectorSource(r_stats_unique$text))

# convert to lower case
twCorpus = tm_map(twCorpus, content_transformer(tolower))

# remove URLs
removeURL = function(x) gsub("http[^[:space:]]*", "", x)
twCorpus = tm_map(twCorpus, content_transformer(removeURL))

# remove anything other than English letters or space
removeNumPunct = function(x) gsub("[^[:alpha:][:space:]]*", "", x)
twCorpus = tm_map(twCorpus, content_transformer(removeNumPunct))

# remove stopwords, as well as other less informative words, including those in the top hashtags and (user) names
twCorpus = tm_map(twCorpus, removeWords, c(stopwords('english'), "kssinglecell",
                                           "many", "using", "will", "amp", "can", "must",
                                           "also", "just", "back", "please", "use", "now",
                                           as.character(ht_df$Var1[ht_df$Freq>1]), "single", "cell",
                                           "cells", "fabiantheis", "danapeer", "scott", "fraser",
                                           tolower(unlist(r_stats_unique$mentions_screen_name))))

# remove extra whitespace
twCorpus = tm_map(twCorpus, stripWhitespace)
```

With this done, we can now make a wordcloud to look at the most frequent
terms. I thought of weighting each term by the retweets and likes each
tweet had, but because there are one or more outlying tweets I opted not
to do it.

``` r
tdm = TermDocumentMatrix(twCorpus)

m = as.matrix(tdm)

word.freq <- sort(rowSums(m), decreasing = T)
pal = viridis::cividis(10)

wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,
          random.order = F, colors = pal)
```

<img src="KStweets_files/figure-markdown_github/unnamed-chunk-9-1.png" style="display: block; margin: auto;" />

Well, it was definitely a great conference. We can also see a few words
floating around though, and it would help us to make them more evident.
We will then cluster the words based on their co-occurrence, to isolate
the most used ones into a cluster.

``` r
adj_m = m[rowSums(m)>2,] %*% t(m[rowSums(m)>2,])

gr = as.undirected(graph_from_adjacency_matrix(adj_m, diag = F, weighted = T), mode = "mutual")

gr_filt = delete.vertices(gr, V(gr)[degree(gr) < 10])

cl = cluster_walktrap(gr_filt, weights = E(gr)$weight, steps = 8)

word_cl = merge(data.frame("word" = cl$names, 
                           "cl" = cl$membership),
                data.frame("word" = rownames(m),
                           "occurrence" = rowSums(m)), by = 1)

top_w = word_cl %>% 
  group_by(cl) %>% 
  top_n(12, occurrence)

plot_list = list()
for(cl in unique(top_w$cl)){
  df = top_w[top_w$cl==cl,]
  df = df[order(df$occurrence, decreasing = T),]
  df$word = factor(df$word, levels = rev(df$word))
  
  plot_list[[cl]] = ggplot(df, aes(x = word, y = occurrence))+
    geom_bar(stat = "identity")+
    coord_flip()+
    labs(title = paste0("Cluster: ", cl))
}

plot_grid(plotlist = plot_list, ncol = 4, nrow = 2)
```

<img src="KStweets_files/figure-markdown_github/unnamed-chunk-10-1.png" style="display: block; margin: auto;" />

This makes it a bit clearer, with the most frequent words concentrated
in 1-2 clusters. Although there is still a lot of noise, we can see more
interesting words popping up in a few clusters, like “spatial”,
“organoids”, “integrating” or “protein”. I would then say that spatial
transcriptomics and multi-modal data integration were the big topics
being discussed.

To finalise, I wanted to contribute something to the discussion, even
without having been there. So to generate a tweet based on this corpus,
I will make a markov chain (see
[here](https://gist.github.com/primaryobjects/4c7cca705eeba0d8bad6) and
[here](https://www.kaggle.com/rtatman/markov-chain-romance-title-generator-in-r))
and then just apply it to see what comes out.

``` r
text = tolower(r_stats_unique$text)
text = gsub("http[^[:space:]]*", " ", text)
text = gsub("#", "aaa", text, fixed = T)
text = gsub("@", "bbb", text, fixed = T)
text = str_replace_all(text, "[[:punct:]]", " ")
text = gsub("aaa", "#", text, fixed = T)
text = gsub("bbb", "@", text, fixed = T)
text = gsub("\n", "", text, fixed = T)
text = strsplit(text, " ")
text = lapply(text, function(x) x[x!=""])

fit <- markovchainFit(data = text)

set.seed(0)
paste(markovchainSequence(n=median(unlist(lapply(text, length))), 
                          markovchain=fit$estimate), collapse=" ")
```

    ## [1] "effects on github immuno saber tomorrow enjoying my research on new multimodal microscopy tools #kssinglecell read based effort including a strong focus on the @keystonesymp next week for correction"

Masterpiece.

Or, for a more condensed and informed summary of the conference, check
out this tweet:  
<blockquote class="twitter-tweet tw-align-center" data-theme="dark" data-link-color="#2B7BB9">
<p lang="en" dir="ltr">
Takeaways from amazing
<a href="https://twitter.com/hashtag/KSsinglecell?src=hash&amp;ref_src=twsrc%5Etfw">\#KSsinglecell</a>:<br>1.
Spatial technologies are mature, and are tremendously exciting<br>2.
Integrating scRNA-seq data with cell morphology, location, and
functional readouts is key for interpretation<br>3. Single cell genomics
is transforming developmental biology
</p>
— Rahul Satija (@satijalab)
<a href="https://twitter.com/satijalab/status/1086289439897182208?ref_src=twsrc%5Etfw">18
de janeiro de 2019</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
